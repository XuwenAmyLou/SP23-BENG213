{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Unsupervised Learning\n",
    "\n",
    "Due Date: Mon, May 8, 2023 at Midnight PST (Submit this .ipynb file on Canvas)\n",
    "\n",
    "Your Name: **Add your name here**\n",
    "\n",
    "<span style=\"color:red\">100 total points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#K-means-clustering\" data-toc-modified-id=\"K-means-clustering-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>K-means clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Describe-K-means\" data-toc-modified-id=\"Describe-K-means-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Describe K-means</a></span></li><li><span><a href=\"#Compute-clusters-of-samples-for-K-=-10\" data-toc-modified-id=\"Compute-clusters-of-samples-for-K-=-10-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Compute clusters of samples for K = 10</a></span><ul class=\"toc-item\"><li><span><a href=\"#Obtain-cluster-labels\" data-toc-modified-id=\"Obtain-cluster-labels-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Obtain cluster labels</a></span></li><li><span><a href=\"#Visualize-the-clusters-with-PCA\" data-toc-modified-id=\"Visualize-the-clusters-with-PCA-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Visualize the clusters with PCA</a></span></li><li><span><a href=\"#Discuss-one-of-the-clusters\" data-toc-modified-id=\"Discuss-one-of-the-clusters-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Discuss one of the clusters</a></span></li></ul></li><li><span><a href=\"#Describe-the-silhouette-coefficient\" data-toc-modified-id=\"Describe-the-silhouette-coefficient-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Describe the silhouette coefficient</a></span></li><li><span><a href=\"#Characterize-the-samples-using-silhouette-coefficients\" data-toc-modified-id=\"Characterize-the-samples-using-silhouette-coefficients-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Characterize the samples using silhouette coefficients</a></span></li><li><span><a href=\"#Choose-an-optimal-K-with-the-Elbow-Method\" data-toc-modified-id=\"Choose-an-optimal-K-with-the-Elbow-Method-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Choose an optimal K with the Elbow Method</a></span></li></ul></li><li><span><a href=\"#Agglomerative-Hierarchical-Clustering\" data-toc-modified-id=\"Agglomerative-Hierarchical-Clustering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Agglomerative Hierarchical Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Describe-Agglomerative-Hierarchical-Clustering\" data-toc-modified-id=\"Describe-Agglomerative-Hierarchical-Clustering-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Describe Agglomerative Hierarchical Clustering</a></span></li><li><span><a href=\"#Compute-Agglomerative-Hierarchical-Clusters\" data-toc-modified-id=\"Compute-Agglomerative-Hierarchical-Clusters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Compute Agglomerative Hierarchical Clusters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-dendrograms-for-two-linkage-types\" data-toc-modified-id=\"Create-dendrograms-for-two-linkage-types-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Create dendrograms for two linkage types</a></span></li><li><span><a href=\"#Obtain-cluster-labels-from-dendrograms\" data-toc-modified-id=\"Obtain-cluster-labels-from-dendrograms-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Obtain cluster labels from dendrograms</a></span></li></ul></li></ul></li><li><span><a href=\"#Principal-Component-Analysis\" data-toc-modified-id=\"Principal-Component-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Principal Component Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Describe-PCA\" data-toc-modified-id=\"Describe-PCA-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Describe PCA</a></span></li><li><span><a href=\"#Covariance\" data-toc-modified-id=\"Covariance-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Covariance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compute-and-visualize-the-covariance-matrix.\" data-toc-modified-id=\"Compute-and-visualize-the-covariance-matrix.-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Compute and visualize the covariance matrix.</a></span></li></ul></li><li><span><a href=\"#PCA-of-the-data\" data-toc-modified-id=\"PCA-of-the-data-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>PCA of the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Highest-weighted-genes-in-PC-1-&amp;-PC-2\" data-toc-modified-id=\"Highest-weighted-genes-in-PC-1-&amp;-PC-2-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Highest weighted genes in PC 1 &amp; PC 2</a></span></li><li><span><a href=\"#Generate-a-biplot\" data-toc-modified-id=\"Generate-a-biplot-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Generate a biplot</a></span></li><li><span><a href=\"#Are-any-PCs-correlated-with-growth-rate?\" data-toc-modified-id=\"Are-any-PCs-correlated-with-growth-rate?-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Are any PCs correlated with growth rate?</a></span></li></ul></li><li><span><a href=\"#Explained-Variance\" data-toc-modified-id=\"Explained-Variance-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Explained Variance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-the-Explained-Variance\" data-toc-modified-id=\"Plot-the-Explained-Variance-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Plot the Explained Variance</a></span></li><li><span><a href=\"#&quot;Denoise&quot;-your-dataset\" data-toc-modified-id=\"&quot;Denoise&quot;-your-dataset-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>\"Denoise\" your dataset</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import cluster, metrics # these will be useful\n",
    "from sklearn import decomposition, metrics # these will be useful\n",
    "\n",
    "# read in the data\n",
    "file_name = 'Homework_Data.xlsx'\n",
    "\n",
    "data = pd.read_excel(file_name, \n",
    "                     sheet_name = 'Expression Data',\n",
    "                     index_col = 0)\n",
    "sample_table = pd.read_excel(file_name,\n",
    "                             sheet_name = 'Experimental Conditions',\n",
    "                             index_col = 0)\n",
    "gene_table = pd.read_excel(file_name,\n",
    "                           sheet_name = 'Gene Information',\n",
    "                           index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "### Describe K-means\n",
    "\n",
    "Use words to describe:\n",
    "\n",
    "1. The overall criteria for K-means clustering\n",
    "2. The algorithm for computing K-means clustering\n",
    "3. Its strengths and weaknesses\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K-means clustering essentially idnetifies k(k defined by user ahead of time) clusters within the data. Whether a sample belong to a certain cluster or not depends on the distance of the sample to the centroids of that specific cluster. It is an algorithm that clusters the samples together based on their distance to the cluster centroids.\n",
    "\n",
    "2. The iterative algorithm will randomly pick k starting points as the initial centroids. During each round, for each sample point, the algorithm calculates its distance to the centroids. The sample point is then clustered to the centroid that it is closest to. After assigning each sample point, the algorithm calculate a new centroid for each cluster based on the sample points that got assigned to it during that round. This is the end of the initial loop. Then the loop repeats itself until specific stopping criteria is met (for example, the assignment no longer changes between rounds).\n",
    "\n",
    "3. The K-means clustering algorithm is advantageous because it is relatively simple to understand and implement (even for large dataset, the code remains similar). It will always give a clustering result in the end (however the quality of the clustering result is not gauranteed). The user can also visually inspect the data and pick the initial starting points if they like (this will likely reduce the runtime).\n",
    "Its weakness is that the user must specify a k value to start with. This means that the user must have some prior knowledge about the grouping of the data, or that will have to experiment to pick the best k value. It is also a method heavily influenced by outliers and initializaiton. The resulting cluster can be difficult to interpret or might not make sense (especially for certain data shape).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute clusters of samples for K = 10\n",
    "\n",
    "#### Obtain cluster labels\n",
    "\n",
    "Compute the K-means clustering with K = 10. You should end up with a pd.Series where each sample is mapped to a cluster number. \n",
    "\n",
    "I recommend specifying a random_state when you perform the clustering (it is a function input). The value you use doesn't matter, but keeping it consistent will allow you to re-run your code without changing the outputs.\n",
    "\n",
    "<span style=\"color:red\">3 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the clusters with PCA\n",
    "\n",
    "You will work directly with PCA later in the assignment. For now, you can use the function provided below to create a PCA plot of your data and color it by the clusters you computed. This creates a two-dimensional plot that captures as much of the 3,887-dimensional gene space as possible. It will therefore give you an idea of where each of your clusters are in gene-space and roughly how much they overlap. \n",
    "\n",
    "For the future, note that this plot is not a complete PCA biplot - it should include the explained variance in the axes labels and have arrows to indicate top gene directions (you will need to add those on your own later in the assignment).\n",
    "\n",
    "<span style=\"color:red\">2 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def PCA_cluster_plot(data, cluster_labels, ax = None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        data: the pd.DataFrame of gene expression data\n",
    "        cluster_labels: a pd.Series with a cluster label\n",
    "            for each column of data\n",
    "        ax: if None, draw a new figure. else, populate these axes.\n",
    "    Returns:\n",
    "        ax: the plt.axes object of a PCA plot labeled by cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    # check for bad input\n",
    "    if isinstance(cluster_labels, pd.Series):\n",
    "        if not(cluster_labels.index.equals(data.columns)):\n",
    "            raise ValueError('cluster_labels.index doesn\\'t match data.columns')\n",
    "    else:\n",
    "        raise TypeError('cluster_labels isn\\'t a pd.Series')\n",
    "    \n",
    "    # compute PCA, save as dataframe with condition labels\n",
    "    data_norm = stats.zscore(data, axis=1)\n",
    "    data_pc = PCA().fit_transform(data_norm.T)\n",
    "    data_pc = pd.DataFrame(data_pc, index = data.columns)\n",
    "    \n",
    "    # get the unique clusters (supports naming if desired)\n",
    "    unique_clusters = np.sort(cluster_labels.unique())\n",
    "    \n",
    "    # prepare a long list of colors\n",
    "    colors = ['tab:blue','tab:orange','tab:green','tab:red','tab:purple',\n",
    "              'tab:brown','tab:pink','tab:gray','tab:olive', 'tab:cyan',\n",
    "              'black', 'salmon', 'chocolate', 'orange', 'gold', 'lawngreen',\n",
    "              'turquoise', 'steelblue', 'navy', 'violet', 'deeppink',\n",
    "              'firebrick', 'sandybrown','olivedrab','darkgreen', 'aqua',\n",
    "              'slategray', 'blue', 'fuschia', 'pink']\n",
    "    \n",
    "    # this code only runs if you have a ton of labels\n",
    "    while len(colors) < len(cluster_labels.unique()):\n",
    "        colors = colors + colors\n",
    "    \n",
    "    # make the plot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('PC1', fontsize = 16)\n",
    "    ax.set_ylabel('PC2', fontsize = 16)\n",
    "    \n",
    "    # iterate through each cluster label\n",
    "    for clust, color in zip(unique_clusters, colors):\n",
    "        \n",
    "        # get the samples of this cluster\n",
    "        samples = cluster_labels.index[cluster_labels == clust]\n",
    "        \n",
    "        # add to plot\n",
    "        ax.scatter(data_pc.loc[samples, 0],\n",
    "                   data_pc.loc[samples, 1],\n",
    "                   label = clust, color = color)\n",
    "    \n",
    "    # add legend\n",
    "    ax.legend(bbox_to_anchor = (1,1))\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss one of the clusters\n",
    "\n",
    "Choose one of the clusters. Display the sample_table entries for the samples in that cluster; is there a common theme or defining characteristic for this cluster? Where does this cluster appear on the PC plot above (e.g. at extremes, in middle, etc), and does its location make sense with its defining characteristics?\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the silhouette coefficient\n",
    "\n",
    "What is the equation for computing the silhouette coefficient? How do you interpret its value? \n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterize the samples using silhouette coefficients\n",
    "\n",
    "Visualize [the silhouette coefficients for each sample](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html) from your K = 10 clustering, being sure to keep the coloring and labeling consistent between this plot and the PCA_cluster_plot() from 1.2.2. \n",
    "\n",
    "Comment on how the silhouette scores relate to the cluster distribution. Note that the 2D PCA plot can't capture everything, and SCs may depend on distances in dimensions that we can't see here.\n",
    "\n",
    "<span style=\"color:red\">10 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an optimal K with the Elbow Method\n",
    "\n",
    "Describe the elbow method and the reasoning behind using it. Using the [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) as your metric, find the best k using the elbow method. Be sure to understand the difference between this use of silhouette scores and the one used above.\n",
    "\n",
    "<span style=\"color:red\">10 points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchical Clustering\n",
    "\n",
    "[This lecture might be helpful.](https://www.andrew.cmu.edu/user/achoulde/95791/lectures/lecture07/lecture07_95791.pdf)\n",
    "\n",
    "### Describe Agglomerative Hierarchical Clustering\n",
    "\n",
    "Use words to describe:\n",
    "\n",
    "1. The overall criteria for agglomerative hierarchical clustering\n",
    "2. The types of linkages (single, complete, & average), what they mean, and how they affect the resulting clusters\n",
    "3. Its strengths and weaknesses\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Agglomerative Hierarchical Clusters\n",
    "\n",
    "#### Create dendrograms for two linkage types\n",
    "\n",
    "Use agglomerative clustering with single and complete linkage types and display *full* dendrograms for each. Comment on how the structure changes.\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain cluster labels from dendrograms\n",
    "\n",
    "Look at the dendrogram for your complete linkage model. Choose 2 different distance_thresholds or n_clusters, then obtain cluster labels for each. Re-use the provided PCA_cluster_plot() function from section 1.2.2 for each of these two sets of clusters. Notice how the change in threshold breaks up the clusters at each step.\n",
    "\n",
    "<span style=\"color:red\">10 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "### Describe PCA\n",
    "\n",
    "Read [\"A tutorial on Principal Component Analysis\" by Jonathon Shlens](https://arxiv.org/pdf/1404.1100.pdf). Summarize:\n",
    "\n",
    "1. The PCA workflow\n",
    "2. Its practical uses\n",
    "3. The weaknesses and assumptions of PCA\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "#### Compute and visualize the covariance matrix.\n",
    "\n",
    "One of the major intermediate steps of computing PCA with eigendecomposition is the construction of the covariance matrix. Compute the covariance matrix for the *genes* in the *normalized* data. Visualize a clustermap of it.\n",
    "\n",
    "Note that the diagonal values should all be equal to 1 since the data has been normalized.\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of the data\n",
    "\n",
    "#### Highest weighted genes in PC 1 & PC 2\n",
    "\n",
    "Compute PCA. List the 5 genes with the highest absolute component weighting from PC 1. Does this suggest a common theme for PC1, and if so, what might it be?\n",
    "\n",
    "Repeat for the top 5 highest absolute weightings from PC 2.\n",
    "\n",
    "What does it mean for a gene to be highly weighted in one of the top components?\n",
    "\n",
    "<span style=\"color:red\">10 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a biplot\n",
    "\n",
    "Generate a biplot of the data, which is like a scatter plot of samples, but also includes a few arrows showing the directions of the genes that contribute most strongly to the first two components. Make sure the explained variance of the PCs are included in the axes labels.\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are any PCs correlated with growth rate?\n",
    "\n",
    "It is interesting to see if the components of a gene expression matrix correlate with phenotypic values. The only quantitative phenotype we have available for this dataset is the growth rate (stored in `sample_table['Growth Rate (1/hr)']`). Iterate through all the PCs and see which one's sample loadings have the highest (absolute value) pearson R correlation with growth rate. Plot the relationship and discuss your results.\n",
    "\n",
    "Growth rate is not available for all samples, so only include the samples that have a matched growth rate.\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance\n",
    "\n",
    "#### Plot the Explained Variance\n",
    "\n",
    "Create a plot of cumulative explained variance for the components in your PCA decomposition. How many components are necessary to explain 80% of the variance in the data?\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Denoise\" your dataset\n",
    "\n",
    "It is sometimes assumed that the least explanatory PCs are the result of experimental noise. Find the number of components needed to explain 95% of the variance in the normalized data, then remove the least explanatory components and reconstruct the normalized dataset without them.\n",
    "\n",
    "Subtract the original normalized dataset from your reconstruction, and visualize the effect of denoising on the values. Is your visualization consistent with the removed effects being only noise?\n",
    "\n",
    "<span style=\"color:red\">5 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
